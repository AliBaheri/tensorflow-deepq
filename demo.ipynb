{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from euclid import Circle, Point2, Vector2, LineSegment2\n",
    "\n",
    "import svg\n",
    "\n",
    "from event_queue import EventQueue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpnbvdvZ\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = tempfile.mkdtemp()\n",
    "print LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tf_models import MLP\n",
    "\n",
    "class DeepQ(object):\n",
    "    def __init__(self, observation_size,\n",
    "                       num_actions,\n",
    "                       observation_to_actions,\n",
    "                       optimizer,\n",
    "                       session,\n",
    "                       random_action_probability=0.05,\n",
    "                       exploration_period=1000,\n",
    "                       minibatch_size=32,\n",
    "                       discount_rate=0.95,\n",
    "                       max_experience=30000,\n",
    "                       summary_writer=None):\n",
    "        \"\"\"Initialized the Deepq object.\n",
    "        \n",
    "        Based on:\n",
    "            https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "            \n",
    "        Parameters\n",
    "        -------\n",
    "        observation_size : int\n",
    "            length of the vector passed as observation\n",
    "        num_actions : int\n",
    "            number of actions that the model can execute\n",
    "        observation_to_actions: dali model\n",
    "            model that implements activate function\n",
    "            that can take in observation vector or a batch\n",
    "            and returns scores (of unbounded values) for each\n",
    "            action for each observation.\n",
    "            input shape:  [batch_size, observation_size]\n",
    "            output shape: [batch_size, num_actions]\n",
    "        optimizer: tf.solver.*\n",
    "            optimizer for prediction error\n",
    "        session: tf.Session\n",
    "            session on which to execute the computation\n",
    "        random_action_probability: float (0 to 1)\n",
    "        exploration_period: int\n",
    "            probability of choosing a random \n",
    "            action (epsilon form paper) annealed linearly\n",
    "            from 1 to random_action_probability over\n",
    "            exploration_period\n",
    "        minibatch_size: int\n",
    "            number of state,action,reward,newstate\n",
    "            tuples considered during experience reply\n",
    "        dicount_rate: float (0 to 1)\n",
    "            how much we care about future rewards.\n",
    "        max_experience: int\n",
    "            maximum size of the reply buffer\n",
    "        summary_writer: tf.train.SummaryWriter\n",
    "            writer to log metrics\n",
    "        \"\"\"\n",
    "        # memorize arguments\n",
    "        self.observation_size          = observation_size\n",
    "        self.num_actions               = num_actions\n",
    "        \n",
    "        self.observation_to_actions    = observation_to_actions\n",
    "        self.optimizer                 = optimizer\n",
    "        self.s                         = session\n",
    "        \n",
    "        self.random_action_probability = random_action_probability\n",
    "        self.exploration_period        = exploration_period\n",
    "        self.minibatch_size            = minibatch_size\n",
    "        self.discount_rate             = tf.constant(discount_rate)\n",
    "        self.max_experience            = max_experience\n",
    "        \n",
    "        # deepq state\n",
    "        self.actions_executed_so_far = 0\n",
    "        self.experience = deque()\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.summary_writer = summary_writer\n",
    "        \n",
    "        self.create_variables()\n",
    "        \n",
    "    def linear_annealing(self, n, total, p_initial, p_final):\n",
    "        \"\"\"Linear annealing between p_initial and p_final\n",
    "        over total steps - computes value at step n\"\"\"\n",
    "        if n >= total:\n",
    "            return p_final\n",
    "        else:\n",
    "            return p_initial - (n * (p_initial - p_final)) / (total)\n",
    "\n",
    "    def create_variables(self):\n",
    "        # FOR REGULAR ACTION SCORE COMPUTATION\n",
    "        with tf.name_scope(\"observation\"):\n",
    "            self.observation        = tf.placeholder(tf.float32, (None, self.observation_size), name=\"observation\")\n",
    "            self.action_scores      = self.observation_to_actions(self.observation)\n",
    "            self.predicted_actions  = tf.argmax(self.action_scores, dimension=1, name=\"predicted_actions\")\n",
    "        \n",
    "        with tf.name_scope(\"future_rewards\"):\n",
    "            # FOR PREDICTING TARGET FUTURE REWARDS\n",
    "            self.observation_mask     = tf.placeholder(tf.float32, (None,), name=\"observation_mask\")        \n",
    "            self.rewards              = tf.placeholder(tf.float32, (None,), name=\"rewards\")\n",
    "            target_values             = tf.reduce_max(self.action_scores, reduction_indices=[1,]) * self.observation_mask\n",
    "            self.future_rewards       = self.rewards + self.discount_rate * target_values\n",
    "        \n",
    "        with tf.name_scope(\"q_value_precition\"):\n",
    "            # FOR PREDICTION ERROR\n",
    "            self.action_mask                = tf.placeholder(tf.float32, (None, self.num_actions))\n",
    "            self.masked_action_scores       = tf.reduce_sum(self.action_scores * self.action_mask, reduction_indices=[1,])\n",
    "            self.precomputed_future_rewards = tf.placeholder(tf.float32, (None,))\n",
    "            temp_diff                       = self.masked_action_scores - self.precomputed_future_rewards\n",
    "            self.prediction_error           = tf.reduce_mean(tf.square(temp_diff))\n",
    "            self.train_op                   = self.optimizer.minimize(self.prediction_error)\n",
    "        \n",
    "        self.metrics = [\n",
    "            tf.scalar_summary(\"prediction_error\", self.prediction_error)\n",
    "        ]\n",
    "        \n",
    "    def action(self, observation):\n",
    "        \"\"\"Given observation returns the action that should be chosen using\n",
    "        DeepQ learning strategy. Does not backprop.\"\"\"\n",
    "        assert len(observation.shape) == 1, \\\n",
    "                \"Action is performed based on single observation.\"\n",
    "\n",
    "        self.actions_executed_so_far += 1\n",
    "        exploration_p = self.linear_annealing(self.actions_executed_so_far,\n",
    "                                              self.exploration_period,\n",
    "                                              1.0,\n",
    "                                              self.random_action_probability)\n",
    "                                                 \n",
    "        if random.random() < exploration_p:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            return self.s.run(self.predicted_actions, {self.observation: observation[np.newaxis,:]})[0]\n",
    "        \n",
    "    def store(self, observation, action, reward, newobservation):\n",
    "        \"\"\"Store experience, where starting with observation and\n",
    "        execution action, we arrived at the newobservation and got the\n",
    "        reward reward\n",
    "        \n",
    "        If newstate is None, the state/action pair is assumed to be terminal\n",
    "        \"\"\"\n",
    "        self.experience.append((observation, action, reward, newobservation))\n",
    "        if len(self.experience) > self.max_experience:\n",
    "            self.experience.popleft()\n",
    "    \n",
    "    def training_step(self):\n",
    "        \"\"\"Pick a self.minibatch_size exeperiences from reply buffer\n",
    "        and backpropage the value function.\n",
    "        \"\"\"\n",
    "        if len(self.experience) <  self.minibatch_size:\n",
    "            return\n",
    "        \n",
    "        # sample experience. \n",
    "        samples   = random.sample(range(len(self.experience)), self.minibatch_size)\n",
    "        samples   = [self.experience[i] for i in samples]\n",
    "        \n",
    "        # bach states\n",
    "        states         = np.empty((len(samples), self.observation_size))\n",
    "        newstates      = np.empty((len(samples), self.observation_size))\n",
    "        action_mask    = np.zeros((len(samples), self.num_actions))\n",
    "        \n",
    "        newstates_mask = np.empty((len(samples),))\n",
    "        rewards        = np.empty((len(samples),))\n",
    "        \n",
    "        for i, (state, action, reward, newstate) in enumerate(samples):\n",
    "            states[i] = state\n",
    "            action_mask[i] = 0\n",
    "            action_mask[i][action] = 1\n",
    "            rewards[i] = reward\n",
    "            if newstate is not None:\n",
    "                newstates[i] = state\n",
    "                newstates_mask[i] = 1\n",
    "            else:\n",
    "                newstates[i] = 0\n",
    "                newstates_mask[i] = 0\n",
    "                \n",
    "\n",
    "        future_rewards = self.s.run(self.future_rewards, {\n",
    "            self.observation:      newstates,\n",
    "            self.observation_mask: newstates_mask,\n",
    "            self.rewards:          rewards,\n",
    "        })\n",
    "        \n",
    "        res = self.s.run([self.prediction_error, self.train_op] + self.metrics, {\n",
    "            self.observation:                states,\n",
    "            self.action_mask:                action_mask,\n",
    "            self.precomputed_future_rewards: future_rewards,\n",
    "        })\n",
    "        cost, metrics = res[0], res[2:]\n",
    "        \n",
    "        if self.summary_writer is not None:\n",
    "            for metric in metrics:\n",
    "                self.summary_writer.add_summary(metric, self.iteration)\n",
    "        self.iteration += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class GameObject(object):\n",
    "    def __init__(self, position, speed, obj_type, settings):\n",
    "        \"\"\"Esentially represents circles of different kinds, which have\n",
    "        position and speed.\"\"\"\n",
    "        self.settings = settings\n",
    "        self.radius = self.settings[\"object_radius\"]\n",
    "        \n",
    "        self.obj_type = obj_type\n",
    "        self.position = position\n",
    "        self.speed    = speed\n",
    "        self.bounciness = 1.0\n",
    "                \n",
    "    def wall_collisions(self):\n",
    "        \"\"\"Update speed upon collision with the wall.\"\"\"\n",
    "        world_size = self.settings[\"world_size\"]\n",
    "\n",
    "        for dim in range(2):\n",
    "            if self.position[dim] - self.radius       <= 0               and self.speed[dim] < 0:\n",
    "                self.speed[dim] = - self.speed[dim] * self.bounciness\n",
    "            elif self.position[dim] + self.radius + 1 >= world_size[dim] and self.speed[dim] > 0:\n",
    "                self.speed[dim] = - self.speed[dim] * self.bounciness\n",
    "        \n",
    "    def move(self, dt):\n",
    "        \"\"\"Move as if dt seconds passed\"\"\"\n",
    "        self.position += dt * self.speed\n",
    "        self.position = Point2(*self.position)\n",
    "        \n",
    "    def step(self, dt):\n",
    "        \"\"\"Move and bounce of walls.\"\"\"\n",
    "        self.wall_collisions()\n",
    "        self.move(dt)\n",
    "        \n",
    "    def as_circle(self):\n",
    "        return Circle(self.position, float(self.radius))\n",
    "        \n",
    "    def draw(self):\n",
    "        \"\"\"Return svg object for this item.\"\"\"\n",
    "        color = self.settings[\"colors\"][self.obj_type]\n",
    "        return svg.Circle(self.position + Point2(10, 10), self.radius, color=color)\n",
    "\n",
    "class KarpathyGame(object):\n",
    "    def __init__(self, settings):\n",
    "        \"\"\"Initiallize game simulator with settings\"\"\"\n",
    "        self.settings = settings\n",
    "        self.size  = self.settings[\"world_size\"]\n",
    "        self.walls = [LineSegment2(Point2(0,0),                        Point2(0,self.size[1])),\n",
    "                      LineSegment2(Point2(0,self.size[1]),             Point2(self.size[0], self.size[1])),\n",
    "                      LineSegment2(Point2(self.size[0], self.size[1]), Point2(self.size[0], 0)),\n",
    "                      LineSegment2(Point2(self.size[0], 0),            Point2(0,0))]\n",
    "        \n",
    "        self.hero = GameObject(Point2(*self.settings[\"hero_initial_position\"]),\n",
    "                               Vector2(*self.settings[\"hero_initial_speed\"]),\n",
    "                               \"hero\",\n",
    "                               self.settings)\n",
    "        if not self.settings[\"hero_bounces_off_walls\"]:\n",
    "            self.hero.bounciness = 0.0\n",
    "        \n",
    "        self.objects = []\n",
    "        for obj_type, number in settings[\"num_objects\"].items():\n",
    "            for _ in range(number):\n",
    "                self.spawn_object(obj_type)\n",
    "        \n",
    "        self.observation_lines = self.generate_observation_lines()\n",
    "                \n",
    "        self.object_reward = 0\n",
    "        self.collected_rewards = []\n",
    "        \n",
    "        # every observation_line sees one of objects or wall and\n",
    "        # two numbers representing speed of the object (if applicable)\n",
    "        self.eye_observation_size = len(self.settings[\"objects\"]) + 3\n",
    "        # additionally there are two numbers representing agents own speed.\n",
    "        self.observation_size = self.eye_observation_size * len(self.observation_lines) + 2\n",
    "        \n",
    "        self.directions = [Vector2(*d) for d in [[1,0], [0,1], [-1,0],[0,-1]]]\n",
    "        self.num_actions      = len(self.directions)\n",
    "        \n",
    "        self.objects_eaten = defaultdict(lambda: 0)\n",
    "        \n",
    "    def perform_action(self, action_id):\n",
    "        \"\"\"Change speed to one of hero vectors\"\"\"\n",
    "        assert 0 <= action_id < self.num_actions\n",
    "        self.hero.speed *= 0.8\n",
    "        self.hero.speed += self.directions[action_id] * self.settings[\"delta_v\"]\n",
    "            \n",
    "    def spawn_object(self, obj_type):\n",
    "        \"\"\"Spawn object of a given type and add it to the objects array\"\"\"\n",
    "        radius = self.settings[\"object_radius\"]\n",
    "        position = np.random.uniform([radius, radius], np.array(self.size) - radius)\n",
    "        position = Point2(float(position[0]), float(position[1]))\n",
    "        max_speed = np.array(self.settings[\"maximum_speed\"])\n",
    "        speed    = np.random.uniform(-max_speed, max_speed).astype(float)\n",
    "        speed = Vector2(float(speed[0]), float(speed[1]))\n",
    "\n",
    "        self.objects.append(GameObject(position, speed, obj_type, self.settings))     \n",
    "                \n",
    "    def step(self, dt):\n",
    "        \"\"\"Simulate all the objects for a given ammount of time.\n",
    "        \n",
    "        Also resolve collisions with the hero\"\"\"\n",
    "        for obj in self.objects + [self.hero] :\n",
    "            obj.step(dt)\n",
    "        self.resolve_collisions()\n",
    "\n",
    "    def squared_distance(self, p1, p2):\n",
    "        return (p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2\n",
    "        \n",
    "    def resolve_collisions(self):\n",
    "        \"\"\"If hero touches, hero eats. Also reward gets updated.\"\"\"\n",
    "        collision_distance = 2 * self.settings[\"object_radius\"]\n",
    "        collision_distance2 = collision_distance ** 2\n",
    "        to_remove = []\n",
    "        for obj in self.objects:\n",
    "            if self.squared_distance(self.hero.position, obj.position) < collision_distance2:\n",
    "                to_remove.append(obj)\n",
    "        for obj in to_remove:\n",
    "            self.objects.remove(obj)\n",
    "            self.objects_eaten[obj.obj_type] += 1\n",
    "            self.object_reward += self.settings[\"object_reward\"][obj.obj_type]\n",
    "            self.spawn_object(obj.obj_type)\n",
    "        \n",
    "    def inside_walls(self, point):\n",
    "        \"\"\"Check if the point is inside the walls\"\"\"\n",
    "        EPS = 1e-4\n",
    "        return (EPS <= point[0] < self.size[0] - EPS and\n",
    "                EPS <= point[1] < self.size[1] - EPS)\n",
    "        \n",
    "    def observe(self):\n",
    "        \"\"\"Return observation vector. For all the observation directions it returns representation\n",
    "        of the closest object to the hero - might be nothing, another object or a wall.\n",
    "        Representation of observation for all the directions will be concatenated.\n",
    "        \"\"\"\n",
    "        num_obj_types = len(self.settings[\"objects\"]) + 1 # and wall\n",
    "        max_speed_x, max_speed_y = self.settings[\"maximum_speed\"]\n",
    "        \n",
    "        observable_distance = self.settings[\"observation_line_length\"]\n",
    "        \n",
    "        relevant_objects = [obj for obj in self.objects \n",
    "                            if obj.position.distance(self.hero.position) < observable_distance]\n",
    "        # objects sorted from closest to furthest\n",
    "        relevant_objects.sort(key=lambda x: x.position.distance(self.hero.position))\n",
    "        \n",
    "        observation        = np.zeros(self.observation_size)\n",
    "        observation_offset = 0 \n",
    "        for i, observation_line in enumerate(self.observation_lines):\n",
    "            # shift to hero position\n",
    "            observation_line = LineSegment2(self.hero.position + Vector2(*observation_line.p1),\n",
    "                                            self.hero.position + Vector2(*observation_line.p2))\n",
    "\n",
    "            observed_object = None\n",
    "            # if end of observation line is outside of walls, we see the wall.\n",
    "            if not self.inside_walls(observation_line.p2):\n",
    "                observed_object = \"**wall**\"\n",
    "            for obj in relevant_objects:\n",
    "                if observation_line.distance(obj.position) < self.settings[\"object_radius\"]:\n",
    "                    observed_object = obj\n",
    "                    break\n",
    "            object_type_id = None\n",
    "            speed_x, speed_y = 0, 0\n",
    "            proximity = 0\n",
    "            if observed_object == \"**wall**\": # wall seen \n",
    "                object_type_id = num_obj_types - 1\n",
    "                # a wall has fairly low speed...\n",
    "                speed_x, speed_y = 0, 0\n",
    "                # best candidate is intersection between\n",
    "                # observation_line and a wall, that's\n",
    "                # closest to the hero\n",
    "                best_candidate = None\n",
    "                for wall in self.walls:\n",
    "                    candidate = observation_line.intersect(wall)\n",
    "                    if candidate is not None:\n",
    "                        if (best_candidate is None or \n",
    "                                best_candidate.distance(self.hero.position) >\n",
    "                                candidate.distance(self.hero.position)):\n",
    "                            best_candidate = candidate\n",
    "                if best_candidate is None:\n",
    "                    # assume it is due to rounding errors\n",
    "                    # and wall is barely touching observation line\n",
    "                    proximity = observable_distance\n",
    "                else:\n",
    "                    proximity = best_candidate.distance(self.hero.position)\n",
    "            elif observed_object is not None: # agent seen\n",
    "                object_type_id = self.settings[\"objects\"].index(observed_object.obj_type)\n",
    "                speed_x, speed_y = tuple(observed_object.speed)\n",
    "                intersection_segment = obj.as_circle().intersect(observation_line)\n",
    "                assert intersection_segment is not None\n",
    "                try:\n",
    "                    proximity = min(intersection_segment.p1.distance(self.hero.position),\n",
    "                                    intersection_segment.p2.distance(self.hero.position))\n",
    "                except AttributeError:\n",
    "                    proximity = observable_distance\n",
    "            for object_type_idx_loop in range(num_obj_types):\n",
    "                observation[observation_offset + object_type_idx_loop] = 1.0\n",
    "            if object_type_id is not None:\n",
    "                observation[observation_offset + object_type_id] = proximity / observable_distance\n",
    "            observation[observation_offset + num_obj_types] =     speed_x   / max_speed_x\n",
    "            observation[observation_offset + num_obj_types + 1] = speed_y   / max_speed_y\n",
    "            assert num_obj_types + 2 == self.eye_observation_size\n",
    "            observation_offset += self.eye_observation_size\n",
    "        \n",
    "        observation[observation_offset]     = self.hero.speed[0] / max_speed_x\n",
    "        observation[observation_offset + 1] = self.hero.speed[1] / max_speed_y\n",
    "        assert observation_offset + 2 == self.observation_size\n",
    "        \n",
    "        return observation        \n",
    "    \n",
    "    def distance_to_walls(self):\n",
    "        \"\"\"Returns distance of a hero to walls\"\"\"\n",
    "        res = float('inf')\n",
    "        for wall in self.walls:\n",
    "            res = min(res, self.hero.position.distance(wall))\n",
    "        return res - self.settings[\"object_radius\"]\n",
    "        \n",
    "    def collect_reward(self):\n",
    "        \"\"\"Return accumulated object eating score + current distance to walls score\"\"\"\n",
    "        wall_reward =  self.settings[\"wall_distance_penalty\"] * \\\n",
    "                       np.exp(-self.distance_to_walls() / self.settings[\"tolerable_distance_to_wall\"])\n",
    "        assert wall_reward < 1e-3, \"You are rewarding hero for being close to the wall!\"\n",
    "        total_reward = wall_reward + self.object_reward\n",
    "        self.object_reward = 0\n",
    "        self.collected_rewards.append(total_reward)\n",
    "        return total_reward\n",
    "        \n",
    "    def plot_reward(self, smoothing = 30):\n",
    "        \"\"\"Plot evolution of reward over time.\"\"\"\n",
    "        plottable = self.collected_rewards[:]\n",
    "        while len(plottable) > 1000:\n",
    "            for i in range(0, len(plottable) - 1, 2):\n",
    "                plottable[i//2] = (plottable[i] + plottable[i+1]) / 2\n",
    "            plottable = plottable[:(len(plottable) // 2)]\n",
    "        x = []\n",
    "        for  i in range(smoothing, len(plottable)):\n",
    "            chunk = plottable[i-smoothing:i]\n",
    "            x.append(sum(chunk) / len(chunk))\n",
    "        plt.plot(list(range(len(x))), x)\n",
    "        \n",
    "    def generate_observation_lines(self):\n",
    "        \"\"\"Generate observation segments in settings[\"num_observation_lines\"] directions\"\"\"\n",
    "        result = []\n",
    "        start = Point2(0.0, 0.0)\n",
    "        end   = Point2(self.settings[\"observation_line_length\"],\n",
    "                       self.settings[\"observation_line_length\"])\n",
    "        for angle in np.linspace(0, 2*np.pi, self.settings[\"num_observation_lines\"], endpoint=False):\n",
    "            rotation = Point2(math.cos(angle), math.sin(angle))\n",
    "            current_start = Point2(start[0] * rotation[0], start[1] * rotation[1])\n",
    "            current_end   = Point2(end[0]   * rotation[0], end[1]   * rotation[1])\n",
    "            result.append( LineSegment2(current_start, current_end))\n",
    "        return result\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.to_html()\n",
    "    \n",
    "    def to_html(self, stats=[]):\n",
    "        \"\"\"Return svg representation of the simulator\"\"\"\n",
    "        scene = svg.Scene((self.size[0] + 20, self.size[1] + 20 + 20 * len(stats)))\n",
    "        scene.add(svg.Rectangle((10, 10), self.size))\n",
    "\n",
    "            \n",
    "        for line in self.observation_lines:\n",
    "            scene.add(svg.Line(line.p1 + self.hero.position + Point2(10,10),\n",
    "                               line.p2 + self.hero.position + Point2(10,10)))\n",
    "        \n",
    "        for obj in self.objects + [self.hero] :\n",
    "            scene.add(obj.draw())\n",
    "        \n",
    "        offset = self.size[1] + 15\n",
    "        for txt in stats:              \n",
    "            scene.add(svg.Text((10, offset + 20), txt, 15))\n",
    "            offset += 20\n",
    "                          \n",
    "        return scene\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import makedirs\n",
    "\n",
    "def simulate(game,\n",
    "             controller,\n",
    "             fps=60,\n",
    "             actions_per_game_second=60,\n",
    "             simulation_resultion=0.001,\n",
    "             speed=1.0,\n",
    "             store_every_nth=5,\n",
    "             train_every_nth=5,\n",
    "             save_path=None):\n",
    "    \"\"\"Start the simulation. Performs three tasks\n",
    "       \n",
    "        - visualizes simulation in iPython notebook\n",
    "        - advances game simulator state\n",
    "        - reports state to controller and chooses actions\n",
    "          to be performed.\n",
    "    \"\"\"\n",
    "    eq = EventQueue()\n",
    "    \n",
    "    time_between_frames  = 1.0 / fps\n",
    "    game_time_between_actions = 1.0 / actions_per_game_second\n",
    "    \n",
    "    simulation_resultion /= speed\n",
    "    \n",
    "    vis_s = {\n",
    "        'last_image': 0\n",
    "    }\n",
    "    \n",
    "    if save_path is not None:\n",
    "        if not exists(save_path):\n",
    "            makedirs(save_path)\n",
    "    \n",
    "    ###### VISUALIZATION\n",
    "    def visualize():\n",
    "        recent_reward = game.collected_rewards[-100:] + [0]\n",
    "        objects_eaten_str = ', '.join([\"%s: %s\" % (o,c) for o,c in game.objects_eaten.items()])\n",
    "        clear_output(wait=True)\n",
    "        svg_html = game.to_html([\n",
    "            \"DTW        = %.1f\" % (game.distance_to_walls(),),\n",
    "            \"experience = %d\" % (len(controller.experience),),\n",
    "            \"reward = %.1f\" % (sum(recent_reward)/len(recent_reward),),\n",
    "            \"objects eaten => %s\" % (objects_eaten_str,),\n",
    "        ])\n",
    "        display(svg_html)\n",
    "        if save_path is not None:\n",
    "            img_path = join(save_path, \"%d.svg\" % (vis_s['last_image'],))\n",
    "            with open(img_path, \"w\") as f:\n",
    "                svg_html.write_svg(f)\n",
    "            vis_s['last_image'] += 1\n",
    "                \n",
    "    eq.schedule_recurring(visualize, time_between_frames)\n",
    "\n",
    "    \n",
    "    ###### CONTROL\n",
    "    ctrl_s = {\n",
    "        'last_observation': None,\n",
    "        'last_action':      None,\n",
    "        'actions_so_far':   0,\n",
    "    }\n",
    "    \n",
    "    def control():\n",
    "        # sense\n",
    "        new_observation = game.observe()\n",
    "        reward          = game.collect_reward()\n",
    "        # store last transition\n",
    "        ctrl_s['actions_so_far'] += 1\n",
    "        if ctrl_s['last_observation'] is not None and ctrl_s['actions_so_far'] % store_every_nth == 0:\n",
    "            controller.store(ctrl_s['last_observation'], ctrl_s['last_action'], reward, new_observation)\n",
    "        # act\n",
    "        new_action = controller.action(new_observation)\n",
    "        game.perform_action(new_action)\n",
    "        ctrl_s['last_action'] = new_action\n",
    "        ctrl_s['last_observation'] = new_observation\n",
    "        \n",
    "        #train\n",
    "        if  ctrl_s['last_observation'] is not None and ctrl_s['actions_so_far'] % train_every_nth == 0:\n",
    "            controller.training_step()\n",
    "    \n",
    "    ##### SIMULATION\n",
    "    sim_s = {\n",
    "        'simulated_up_to':             time.time(),\n",
    "        'game_time_since_last_action': 0,\n",
    "    }\n",
    "    def simulate_game():\n",
    "        while sim_s['simulated_up_to'] < time.time():\n",
    "            game.step(simulation_resultion)\n",
    "            sim_s['simulated_up_to'] += simulation_resultion / speed\n",
    "            sim_s['game_time_since_last_action'] += simulation_resultion\n",
    "            if sim_s['game_time_since_last_action'] > game_time_between_actions:\n",
    "                control()\n",
    "                sim_s['game_time_since_last_action'] = 0\n",
    "        \n",
    "    eq.schedule_recurring(simulate_game, time_between_frames)\n",
    "    \n",
    "    eq.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_settings = {\n",
    "    'objects': [\n",
    "        'friend',\n",
    "        'enemy',\n",
    "         'boss'\n",
    "    ],\n",
    "    'colors': {\n",
    "        'hero':   'yellow',\n",
    "        'friend': 'green',\n",
    "        'enemy':  'red',\n",
    "         'boss':   'orange',\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'friend': 0.1,\n",
    "        'enemy': -0.1,\n",
    "        'boss':  -0.5,\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (700,500),\n",
    "    'hero_initial_position': [400, 300],\n",
    "    'hero_initial_speed':    [0,   0],\n",
    "    \"maximum_speed\":         [50, 50],\n",
    "    \"object_radius\": 10.0,\n",
    "    \"num_objects\": {\n",
    "        \"friend\" : 25,\n",
    "        \"enemy\" :  25,\n",
    "        \"boss\" :   5,\n",
    "    },\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 120.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -0.0,\n",
    "    \"delta_v\": 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f5b90753190>> ignored\n",
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f5bc457f550>> ignored\n"
     ]
    }
   ],
   "source": [
    "tf.ops.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# brain maps from observation to action q values. Here it is a simple mlp\n",
    "brain = MLP([g.observation_size,], [100, 100, g.num_actions], \n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "# solver over brian - here simple sgd\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.0001, decay=0.9)\n",
    "\n",
    "# DeepQ object\n",
    "current_controller = DeepQ(g.observation_size, g.num_actions, brain, optimizer, session,\n",
    "                           discount_rate=0.9, exploration_period=5000, max_experience=10000, summary_writer=journalist)\n",
    "session.run(tf.initialize_all_variables())\n",
    "journalist.add_graph(session.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT FOR WSAD CONTROL (requires redis server to be running, commands can be sent from terminal)\n",
    "# from human_control import HumanController\n",
    "# current_controller = HumanController({b\"w\": 3, b\"d\": 0, b\"s\": 1,b\"a\": 2,}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-347293524ff8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m          \u001b[0mstore_every_nth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m          \u001b[0mtrain_every_nth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m          save_path=\"my_sim\")\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-bee82a03dffd>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(game, controller, fps, actions_per_game_second, simulation_resultion, speed, store_every_nth, train_every_nth, save_path)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0meq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule_recurring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimulate_game\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_between_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0meq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/sidor/projects/dali-deepq/event_queue.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sidor/projects/dali-deepq/event_queue.pyc\u001b[0m in \u001b[0;36mrecuring_f\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m         from now\"\"\"\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mrecuring_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecuring_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecuring_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-bee82a03dffd>\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;34m\"experience = %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;34m\"reward = %.1f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecent_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecent_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;34m\"objects eaten => %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobjects_eaten_str\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         ])\n\u001b[0;32m     46\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvg_html\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9169de419752>\u001b[0m in \u001b[0;36mto_html\u001b[1;34m(self, stats)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             scene.add(svg.Line(line.p1 + self.hero.position + Point2(10,10),\n\u001b[0m\u001b[0;32m    263\u001b[0m                                line.p2 + self.hero.position + Point2(10,10)))\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/euclid.pyc\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVector2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m             \u001b[1;31m# Vector + Vector -> Vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;31m# Vector + Point -> Point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THE LINE BELOW IS FOR FAST LEARNING\n",
    "# FPS, SPEED, RES = 1, 4.5, 0.1\n",
    "# THE LINE BELOW IS FOR REAL TIME VISUALIZATION\n",
    "FPS, SPEED, RES = 30, 1., 0.03\n",
    "\n",
    "simulate(g, current_controller, fps = FPS,\n",
    "         simulation_resultion=RES,\n",
    "         actions_per_game_second=10,\n",
    "         speed=SPEED,\n",
    "         store_every_nth=4,\n",
    "         train_every_nth=4,\n",
    "         save_path=\"my_sim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ba864b950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g.plot_reward(smoothing=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what the agent is seeing\n",
    "\n",
    "Starting with the ray pointing all the way right, we have one row per ray in clockwise order.\n",
    "The numbers for each ray are the following:\n",
    "- first three numbers are normalized distances to the closest visible (intersecting with the ray) object. If no object is visible then all of them are $1$. If there's many objects in sight, then only the closest one is visible. The numbers represent distance to friend, enemy and wall in order.\n",
    "- the last two numbers represent the speed of moving object (x and y components). Speed of wall is ... zero.\n",
    "\n",
    "Finally the last two numbers in the representation correspond to speed of the hero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00 0.55 1.00 0.49 -0.66]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [0.34 1.00 1.00 0.18 0.01]\n",
      " [0.34 1.00 1.00 0.18 0.01]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 0.22 0.00 0.00]\n",
      " [1.00 1.00 0.11 0.00 0.00]\n",
      " [1.00 1.00 0.08 0.00 0.00]\n",
      " [1.00 1.00 0.06 0.00 0.00]\n",
      " [1.00 1.00 0.05 0.00 0.00]\n",
      " [1.00 1.00 0.05 0.00 0.00]\n",
      " [1.00 1.00 0.04 0.00 0.00]\n",
      " [1.00 1.00 0.04 0.00 0.00]\n",
      " [1.00 1.00 0.04 0.00 0.00]\n",
      " [1.00 1.00 0.05 0.00 0.00]\n",
      " [1.00 1.00 0.05 0.00 0.00]\n",
      " [1.00 1.00 0.06 0.00 0.00]\n",
      " [1.00 1.00 0.08 0.00 0.00]\n",
      " [1.00 1.00 0.11 0.00 0.00]\n",
      " [1.00 1.00 0.22 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [0.77 1.00 1.00 -0.65 -0.11]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 1.00 1.00 0.00 0.00]\n",
      " [1.00 0.54 1.00 0.49 -0.66]]\n",
      "[0.00 0.11]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<?xml version=\"1.0\"?>\n",
       "\n",
       "<svg height=\"520\" width=\"720\" >\n",
       "\n",
       " <g style=\"fill-opacity:1.0; stroke:black;\n",
       "\n",
       "  stroke-width:1;\">\n",
       "\n",
       "  <rect x=\"10\" y=\"10\" height=\"500\"\n",
       "\n",
       "        width=\"700\" style=fill:none; />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"135\" y2=\"228\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"132\" y2=\"252\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"125\" y2=\"274\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"114\" y2=\"295\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"99\" y2=\"313\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"81\" y2=\"328\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"60\" y2=\"339\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"38\" y2=\"346\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"15\" y2=\"348\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-8\" y2=\"346\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-30\" y2=\"339\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-51\" y2=\"328\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-69\" y2=\"313\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-84\" y2=\"295\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-95\" y2=\"274\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-102\" y2=\"252\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-104\" y2=\"228\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-102\" y2=\"205\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-95\" y2=\"183\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-84\" y2=\"162\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-69\" y2=\"144\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-51\" y2=\"129\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-30\" y2=\"118\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"-8\" y2=\"111\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"15\" y2=\"108\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"38\" y2=\"111\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"60\" y2=\"118\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"81\" y2=\"129\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"99\" y2=\"144\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"114\" y2=\"162\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"125\" y2=\"183\" />\n",
       "\n",
       "  <line x1=\"15\" y1=\"228\" x2=\"132\" y2=\"205\" />\n",
       "\n",
       "  <circle cx=\"25\" cy=\"132\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"167\" cy=\"364\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"299\" cy=\"251\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"665\" cy=\"93\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"669\" cy=\"102\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"443\" cy=\"363\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"176\" cy=\"361\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"138\" cy=\"401\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"305\" cy=\"109\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"53\" cy=\"260\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"475\" cy=\"297\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"87\" cy=\"221\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"157\" cy=\"263\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"594\" cy=\"319\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"672\" cy=\"335\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"678\" cy=\"128\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"469\" cy=\"491\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"431\" cy=\"77\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"534\" cy=\"389\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"231\" cy=\"444\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"570\" cy=\"230\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"633\" cy=\"309\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"273\" cy=\"387\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"250\" cy=\"200\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"686\" cy=\"110\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"656\" cy=\"177\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"129\" cy=\"38\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"632\" cy=\"29\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"356\" cy=\"162\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"368\" cy=\"400\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"655\" cy=\"368\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"559\" cy=\"135\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"262\" cy=\"363\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"510\" cy=\"76\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"416\" cy=\"96\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"286\" cy=\"259\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"116\" cy=\"470\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"680\" cy=\"42\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"387\" cy=\"187\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"484\" cy=\"480\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"218\" cy=\"276\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"291\" cy=\"95\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"507\" cy=\"446\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"471\" cy=\"303\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"500\" cy=\"143\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"454\" cy=\"341\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"677\" cy=\"303\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"508\" cy=\"103\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"168\" cy=\"197\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"466\" cy=\"376\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"15\" cy=\"228\" r=\"10\"\n",
       "\n",
       "          style=fill:yellow; />\n",
       "\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<svg.Scene at 0x7f5a7704f128>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.__class__ = KarpathyGame\n",
    "np.set_printoptions(formatter={'float': (lambda x: '%.2f' % (x,))})\n",
    "x = g.observe()\n",
    "new_shape = (x[:-2].shape[0]//g.eye_observation_size, g.eye_observation_size)\n",
    "print(x[:-2].reshape(new_shape))\n",
    "print(x[-2:])\n",
    "g.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = g.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"siema.svg\", \"w\") as s:\n",
    "    h.write_svg(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
