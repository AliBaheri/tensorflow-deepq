{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dali.core as D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from dali.models import MLP\n",
    "from dali.utils import Solver\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from sympy.geometry import Point, Circle, Segment, Polygon\n",
    "\n",
    "import svg\n",
    "\n",
    "from event_queue import EventQueue\n",
    "from geometry import point_distance, point_projected_on_line, point_segment_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D.config.default_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DeepQ(object):\n",
    "    def __init__(self, observation_size,\n",
    "                       num_actions,\n",
    "                       observation_to_actions,\n",
    "                       solver,\n",
    "                       random_action_probability=0.05,\n",
    "                       exploration_period=1000,\n",
    "                       minibatch_size=30,\n",
    "                       discount_rate=0.95,\n",
    "                       max_experience=20000):\n",
    "        \"\"\"Initialized the Deepq object.\n",
    "        \n",
    "        Based on:\n",
    "            https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "            \n",
    "        Parameters\n",
    "        -------\n",
    "        observation_size : int\n",
    "            length of the vector passed as observation\n",
    "        num_actions : int\n",
    "            number of actions that the model can execute\n",
    "        observation_to_actions: dali model\n",
    "            model that implements activate function\n",
    "            that can take in observation vector or a batch\n",
    "            and returns scores (of unbounded values) for each\n",
    "            action for each observation.\n",
    "            input shape:  [batch_size, observation_size]\n",
    "            output shape: [batch_size, num_actions]\n",
    "        solver: dali solver\n",
    "            solver over observation_to_actions,\n",
    "            must implement the step function\n",
    "        random_action_probability: float (0 to 1)\n",
    "        exploration_period: int\n",
    "            probability of choosing a random \n",
    "            action (epsilon form paper) annealed linearly\n",
    "            from 1 to random_action_probability over\n",
    "            exploration_period\n",
    "        minibatch_size: int\n",
    "            number of state,action,reward,newstate\n",
    "            tuples considered during experience reply\n",
    "        dicount_rate: float (0 to 1)\n",
    "            how much we care about future rewards.\n",
    "        max_experience: int\n",
    "            maximum size of the reply buffer\n",
    "        \"\"\"\n",
    "        # memorize arguments\n",
    "        self.observation_size          = observation_size\n",
    "        self.num_actions               = num_actions\n",
    "        \n",
    "        self.observation_to_actions    = observation_to_actions\n",
    "        self.solver                    = solver\n",
    "        \n",
    "        self.random_action_probability = random_action_probability\n",
    "        self.exploration_period        = exploration_period\n",
    "        self.minibatch_size            = minibatch_size\n",
    "        self.discount_rate             = discount_rate\n",
    "        self.max_experience            = max_experience\n",
    "        \n",
    "        # deepq state\n",
    "        self.actions_executed_so_far = 0\n",
    "        self.experience = deque()\n",
    "        \n",
    "    def linear_annealing(self, n, total, p_initial, p_final):\n",
    "        \"\"\"Linear annealing between p_initial and p_final\n",
    "        over total steps - computes value at step n\"\"\"\n",
    "        if n >= total:\n",
    "            return p_final\n",
    "        else:\n",
    "            return p_initial - (n * (p_initial - p_final)) / (total)\n",
    "\n",
    "    def activate(self, observation):\n",
    "        \"\"\"Given observation or a batch returns action scores.\"\"\"\n",
    "        action_scores = self.observation_to_actions.activate(observation)\n",
    "        assert action_scores.shape[1] == self.num_actions, \\\n",
    "                \"number of columns in the output of `observation_to_actions` must be equal to number of actions.\"\n",
    "        assert action_scores.shape[0] == observation.shape[0], \\\n",
    "                \"number of output rows of `observation_to_actions` must be equal to number of input rows\"\n",
    "        return action_scores\n",
    "    \n",
    "    def action(self, observation):\n",
    "        \"\"\"Given observation returns the action that should be chosen using\n",
    "        DeepQ learning strategy. Does not backprop.\"\"\"\n",
    "        assert len(observation.shape) == 1, \\\n",
    "                \"Action is performed based on single observation.\"\n",
    "\n",
    "        self.actions_executed_so_far += 1\n",
    "        exploration_p = self.linear_annealing(self.actions_executed_so_far,\n",
    "                                              self.exploration_period,\n",
    "                                              1.0,\n",
    "                                              self.random_action_probability)\n",
    "                                                 \n",
    "        if random.random() < exploration_p:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            with D.NoBackprop():\n",
    "                observation_dali = D.Mat(observation[np.newaxis,:], constant=True)\n",
    "                assert observation_dali.shape == (1, observation.shape[0])\n",
    "                action_scores = self.activate(observation_dali)\n",
    "                return D.MatOps.argmax(action_scores, axis=1)[0]\n",
    "        \n",
    "    def store(self, observation, action, reward, newobservation):\n",
    "        \"\"\"Store experience, where starting with observation and\n",
    "        execution action, we arrived at the newobservation and got the\n",
    "        reward reward\n",
    "        \n",
    "        If newstate is None, the state/action pair is assumed to be terminal\n",
    "        \"\"\"\n",
    "        self.experience.append((observation, action, reward, newobservation))\n",
    "        if len(self.experience) > self.max_experience:\n",
    "            self.experience.popleft()\n",
    "    \n",
    "    def training_step(self):\n",
    "        \"\"\"Pick a self.minibatch_size exeperiences from reply buffer\n",
    "        and backpropage the value function.\n",
    "        \"\"\"\n",
    "        if len(self.experience) <  self.minibatch_size:\n",
    "            return\n",
    "        \n",
    "        # sample experience. \n",
    "        samples   = random.sample(range(len(self.experience)), self.minibatch_size)\n",
    "        samples   = [self.experience[i] for i in samples]\n",
    "        \n",
    "        # bach states\n",
    "        states    = np.empty((len(samples), self.observation_size))\n",
    "        newstates = np.empty((len(samples), self.observation_size))\n",
    "        action_mask    = np.zeros((len(samples), self.num_actions))\n",
    "        \n",
    "        newstates_mask = np.empty((len(samples),))\n",
    "        rewards        = np.empty((len(samples),))\n",
    "        \n",
    "        for i, (state, action, reward, newstate) in enumerate(samples):\n",
    "            states[i] = state\n",
    "            action_mask[i] = 0\n",
    "            action_mask[i][action] = 1\n",
    "            rewards[i] = reward\n",
    "            if newstate is not None:\n",
    "                newstates[i] = state\n",
    "                newstates_mask[i] = 1\n",
    "            else:\n",
    "                newstates[i] = 0\n",
    "                newstates_mask[i] = 0\n",
    "                \n",
    "        # convert to dali, steal numpy memory, do not compute gradient (that's constant)\n",
    "        states      = D.Mat(states,      borrow=True, constant=True)\n",
    "        newstates   = D.Mat(newstates,   borrow=True, constant=True)\n",
    "        action_mask = D.Mat(action_mask, borrow=True, constant=True)\n",
    "        \n",
    "        # compute target value functions\n",
    "        with D.NoBackprop():\n",
    "            action_scores = self.activate(newstates)\n",
    "        # rowwise max - best achievable value function for each sample.\n",
    "        newstates_value = action_scores.w.max(axis=1) * newstates_mask\n",
    "        targets = rewards + self.discount_rate * newstates_value\n",
    "\n",
    "        # convert to dali, steal numpy memory, do not compute gradient (that's constant)\n",
    "        targets     = D.Mat(targets,     borrow=True, constant=True)\n",
    "        \n",
    "        # this computation will be backpropagated.\n",
    "        action_scores = self.activate(states)\n",
    "        relevant_actions = (action_scores * action_mask).sum(axis=1)\n",
    "        error = (relevant_actions - targets)**2\n",
    "        error.grad()\n",
    "        \n",
    "        # compute gradient\n",
    "        D.Graph.backward()\n",
    "        # apply gradient\n",
    "        solver.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class GameObject(object):\n",
    "    def __init__(self, position, speed, obj_type, settings):\n",
    "        \"\"\"Esentially represents circles of different kinds, which have\n",
    "        position and speed.\"\"\"\n",
    "        self.settings = settings\n",
    "        self.radius = self.settings[\"object_radius\"]\n",
    "        \n",
    "        self.obj_type = obj_type\n",
    "        self.position = np.array(position, dtype=float)\n",
    "        self.speed = np.array(speed, dtype=float)\n",
    "        \n",
    "    def wall_collisions(self):\n",
    "        \"\"\"Update speed upon collision with the wall.\"\"\"\n",
    "        world_size = self.settings[\"world_size\"]\n",
    "        for dim in range(2):\n",
    "            if self.position[dim] - self.radius       <= 0               and self.speed[dim] < 0:\n",
    "                self.speed[dim] = - self.speed[dim]\n",
    "            elif self.position[dim] + self.radius + 1 >= world_size[dim] and self.speed[dim] > 0:\n",
    "                self.speed[dim] = - self.speed[dim]\n",
    "        \n",
    "    def move(self, dt):\n",
    "        \"\"\"Move as if dt seconds passed\"\"\"\n",
    "        self.position += dt * self.speed\n",
    "        \n",
    "    def step(self, dt):\n",
    "        \"\"\"Move and bounce of walls.\"\"\"\n",
    "        self.wall_collisions()\n",
    "        self.move(dt)\n",
    "        \n",
    "    def draw(self):\n",
    "        \"\"\"Return svg object for this item.\"\"\"\n",
    "        color = self.settings[\"colors\"][self.obj_type]\n",
    "        return svg.Circle(self.position.astype(int) + 10, self.radius, color=color)\n",
    "\n",
    "class KarpathyGame(object):\n",
    "    def __init__(self, settings):\n",
    "        \"\"\"Initiallize game simulator with settings\"\"\"\n",
    "        self.settings = settings\n",
    "        self.size = np.array(self.settings[\"world_size\"])\n",
    "        \n",
    "        self.hero = GameObject(self.settings[\"hero_initial_position\"].copy(),\n",
    "                               self.settings[\"hero_initial_speed\"].copy(),\n",
    "                               \"hero\",\n",
    "                               self.settings)\n",
    "        \n",
    "        self.objects = []\n",
    "        for obj_type, number in settings[\"num_objects\"].items():\n",
    "            for _ in range(number):\n",
    "                self.spawn_object(obj_type)\n",
    "        \n",
    "        self.observation_lines = self.generate_observation_lines()\n",
    "                \n",
    "        self.object_reward = 0\n",
    "        self.collected_rewards = []\n",
    "        \n",
    "        # every observation_line sees one of objects or wall and\n",
    "        # two numbers representing speed of the object (if applicable)\n",
    "        # and one number representing proximity\n",
    "        self.eye_observation_size = len(self.settings[\"objects\"]) + 4\n",
    "        # additionally there are two numbers representing agents own speed.\n",
    "        self.observation_size = self.eye_observation_size * len(self.observation_lines) + 2\n",
    "        \n",
    "        self.directions = [np.array(d) for d in [[1,0], [0,1], [-1,0],[0,-1]]]\n",
    "        self.num_actions      = len(self.directions)\n",
    "        \n",
    "        self.objects_eaten = defaultdict(lambda: 0)\n",
    "        \n",
    "    def perform_action(self, action_id):\n",
    "        \"\"\"Change speed to one of hero vectors\"\"\"\n",
    "        assert 0 <= action_id < self.num_actions\n",
    "        self.hero.speed *= 0.95\n",
    "        self.hero.speed += self.directions[action_id] * self.settings[\"delta_v\"]\n",
    "            \n",
    "    def spawn_object(self, obj_type):\n",
    "        \"\"\"Spawn object of a given type and add it to the objects array\"\"\"\n",
    "        radius = self.settings[\"object_radius\"]\n",
    "        position = np.random.uniform([radius, radius], self.size - radius).astype(float)\n",
    "        speed    = np.random.uniform([-100,-100], [100,100]).astype(float)\n",
    "        self.objects.append(GameObject(position, speed, obj_type, self.settings))     \n",
    "                \n",
    "    def step(self, dt):\n",
    "        \"\"\"Simulate all the objects for a given ammount of time.\n",
    "        \n",
    "        Also resolve collisions with the hero\"\"\"\n",
    "        for obj in self.objects + [self.hero] :\n",
    "            obj.step(dt)\n",
    "        self.resolve_collisions()\n",
    "\n",
    "    def resolve_collisions(self):\n",
    "        \"\"\"If hero touches, hero eats. Also reward gets updated.\"\"\"\n",
    "        to_remove = []\n",
    "        for obj in self.objects:\n",
    "            if np.linalg.norm(obj.position - self.hero.position) < 2 * self.settings[\"object_radius\"]:\n",
    "                to_remove.append(obj)\n",
    "        for obj in to_remove:\n",
    "            self.objects.remove(obj)\n",
    "            self.objects_eaten[obj.obj_type] += 1\n",
    "            self.object_reward += self.settings[\"object_reward\"][obj.obj_type]\n",
    "            self.spawn_object(obj.obj_type)\n",
    "        \n",
    "    def inside_walls(self, point):\n",
    "        \"\"\"Check if the point is inside the walls\"\"\"\n",
    "        return np.all(np.array([0,0]) <= point) and np.all(point < self.size)\n",
    "        \n",
    "    def observe(self):\n",
    "        \"\"\"Return observation vector. For all the observation directions it returns representation\n",
    "        of the closest object to the hero - might be nothing, another object or a wall.\n",
    "        Representation of observation for all the directions will be concatenated.\n",
    "        \"\"\"\n",
    "        num_obj_types = len(self.settings[\"objects\"]) + 1 # and wall\n",
    "\n",
    "        observable_distance = self.settings[\"object_radius\"] + self.settings[\"observation_line_length\"]\n",
    "        relevant_objects = [obj for obj in self.objects \n",
    "                            if point_distance(obj.position, self.hero.position) < observable_distance]\n",
    "        # objects sorted from closest to furthest\n",
    "        relevant_objects.sort(key=lambda x: point_distance(x.position, self.hero.position))\n",
    "        \n",
    "        observation        = np.zeros(self.observation_size)\n",
    "        observation_offset = 0 \n",
    "        for i, (line_start, line_end) in enumerate(self.observation_lines):\n",
    "            line_start = line_start + self.hero.position\n",
    "            line_end   = line_end   + self.hero.position\n",
    "            observed_object = None\n",
    "            if not self.inside_walls(line_end):\n",
    "                observed_object = \"**wall**\"\n",
    "            for obj in relevant_objects:\n",
    "                if point_segment_distance(line_start, line_end, obj.position) < SETTINGS[\"object_radius\"]:\n",
    "                    observed_object = obj\n",
    "                    break\n",
    "            object_type_id = None\n",
    "            speed_x, speed_y = 0, 0\n",
    "            proximity = 0\n",
    "            if observed_object == \"**wall**\": # wall seen \n",
    "                object_type_id = num_obj_types - 1\n",
    "                # wall has a fairly low speed of movement...\n",
    "                speed_x, speed_y = 0, 0\n",
    "                # TODO: line below is not entirely correct\n",
    "                # different eyes can see different distance\n",
    "                proximity = self.distance_to_walls()\n",
    "            elif observed_object is not None: # agent seen\n",
    "                object_type_id = self.settings[\"objects\"].index(observed_object.obj_type)\n",
    "                speed_x, speed_y = observed_object.speed\n",
    "                print (self.hero.position, obj.position)\n",
    "                proximity = max(point_distance(self.hero.position, obj.position)\n",
    "                                - 2.0 * self.settings[\"object_radius\"], 0)\n",
    "                print(proximity)\n",
    "            if object_type_id is not None:\n",
    "                observation[observation_offset + object_type_id] = 1\n",
    "            observation[observation_offset + num_obj_types] = speed_x\n",
    "            observation[observation_offset + num_obj_types + 1] = speed_y\n",
    "            observation[observation_offset + num_obj_types + 2] = proximity\n",
    "            assert num_obj_types + 3 == self.eye_observation_size\n",
    "            observation_offset += self.eye_observation_size\n",
    "        \n",
    "        observation[observation_offset]     = self.hero.speed[0]\n",
    "        observation[observation_offset + 1] = self.hero.speed[1]\n",
    "        assert observation_offset + 2 == self.observation_size\n",
    "        \n",
    "        return observation        \n",
    "    \n",
    "    def distance_to_walls(self):\n",
    "        \"\"\"Returns distance of a hero to walls\"\"\"\n",
    "        tl = np.min(self.hero.position - self.settings[\"object_radius\"])\n",
    "        br = np.min(self.size - self.hero.position - self.settings[\"object_radius\"])\n",
    "        return np.min([tl,br])\n",
    "        \n",
    "    def collect_reward(self):\n",
    "        \"\"\"Return accumulated object eating score + current distance to walls score\"\"\"\n",
    "        wall_reward =  self.settings[\"wall_distance_penalty\"] * \\\n",
    "                       np.exp(-self.distance_to_walls() / self.settings[\"tolerable_distance_to_wall\"])\n",
    "        total_reward = wall_reward + self.object_reward\n",
    "        self.object_reward = 0\n",
    "        self.collected_rewards.append(total_reward)\n",
    "        return total_reward\n",
    "        \n",
    "    def plot_reward(self, smoothing = 30):\n",
    "        \"\"\"Plot evolution of reward over time.\"\"\"\n",
    "        plottable = self.collected_rewards[:]\n",
    "        while len(plottable) > 1000:\n",
    "            for i in range(0, len(plottable) - 1, 2):\n",
    "                plottable[i//2] = (plottable[i] + plottable[i+1]) / 2\n",
    "            plottable = plottable[:(len(plottable) // 2)]\n",
    "        x = []\n",
    "        for  i in range(smoothing, len(plottable)):\n",
    "            chunk = plottable[i-smoothing:i]\n",
    "            x.append(sum(chunk) / len(chunk))\n",
    "        plt.plot(list(range(len(x))), x)\n",
    "        \n",
    "    def generate_observation_lines(self):\n",
    "        \"\"\"Generate observation segments in settings[\"num_observation_lines\"] directions\"\"\"\n",
    "        result = []\n",
    "        start = np.array([self.settings[\"object_radius\"]* 1.1, self.settings[\"object_radius\"]* 1.1])\n",
    "        end   = np.array([self.settings[\"object_radius\"] + self.settings[\"observation_line_length\"],\n",
    "                          self.settings[\"object_radius\"] + self.settings[\"observation_line_length\"]])\n",
    "        for angle in np.linspace(0, 2*np.pi, self.settings[\"num_observation_lines\"], endpoint=False):\n",
    "            rotation = np.array([np.cos(angle), np.sin(angle)])\n",
    "            result.append((start * rotation, end * rotation))\n",
    "        return result\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.to_html()\n",
    "    \n",
    "    def to_html(self, stats):\n",
    "        \"\"\"Return svg representation of the simulator\"\"\"\n",
    "        scene = svg.Scene((self.size[0] + 20, self.size[1] + 20 + 20 * len(stats)))\n",
    "        scene.add(svg.Rectangle((10, 10), self.size))\n",
    "        for obj in self.objects + [self.hero] :\n",
    "            scene.add(obj.draw())\n",
    "            \n",
    "        for line_start, line_end in self.observation_lines:\n",
    "            scene.add(svg.Line(line_start + self.hero.position + 10, line_end + self.hero.position + 10))\n",
    "        \n",
    "        offset = self.size[1] + 15\n",
    "        for txt in stats:              \n",
    "            scene.add(svg.Text((10, offset + 20), txt, 15))\n",
    "            offset += 20\n",
    "                          \n",
    "        return scene\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    'objects': [\n",
    "        'friend',\n",
    "        'enemy',\n",
    "#         'boss'\n",
    "    ],\n",
    "    'colors': {\n",
    "        'hero':   'yellow',\n",
    "        'friend': 'green',\n",
    "        'enemy':  'red',\n",
    "#         'boss':   'orange',\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'friend': 5,\n",
    "        'enemy': -5,\n",
    "#         'boss':  -20,\n",
    "    },\n",
    "    'world_size': (800,600),\n",
    "    'hero_initial_position': [400, 300],\n",
    "    'hero_initial_speed':    [0, 0],\n",
    "    \"object_radius\": 10,\n",
    "    \"num_objects\": {\n",
    "        \"friend\" : 10,\n",
    "        \"enemy\" :  10,\n",
    "#         \"boss\" :   5,\n",
    "    },\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 100,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  0.1,\n",
    "    \"delta_v\": 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the game simulator\n",
    "g = KarpathyGame(SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# brain maps from observation to action q values. Here it is a simple mlp\n",
    "brain = MLP([g.observation_size,], [200, 100, g.num_actions], [D.MatOps.tanh, D.MatOps.tanh, lambda x: x])\n",
    "# solver over brian - here simple sgd\n",
    "# solver = Solver(brain.parameters(), \"rmsprop\", learning_rate= 0.005, decay_rate = 0.999, smooth_eps = 1e-8;)\n",
    "solver = Solver(brain.parameters(), \"sgd\", learning_rate= 0.01)\n",
    "# DeepQ object\n",
    "c = DeepQ(g.observation_size, g.num_actions, brain, solver, exploration_period=5000, max_experience=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate(game,\n",
    "             controller,\n",
    "             fps=60,\n",
    "             actions_per_second=60,\n",
    "             simulation_resultion=0.001,\n",
    "             speed=1.0,\n",
    "             store_every_nth=5):\n",
    "    \"\"\"Start the simulation. Performs three tasks\n",
    "       \n",
    "        - visualizes simulation in iPython notebook\n",
    "        - advances game simulator state\n",
    "        - reports state to controller and chooses actions\n",
    "          to be performed.\n",
    "    \"\"\"\n",
    "    eq = EventQueue()\n",
    "    \n",
    "    time_between_frames  = 1.0 / fps\n",
    "    time_between_actions = 1.0 / (actions_per_second * speed)\n",
    "    \n",
    "    def visualize():\n",
    "        clear_output(wait=True)\n",
    "        recent_reward = game.collected_rewards[-100:] + [0]\n",
    "        objects_eaten_str = ', '.join([\"%s: %s\" % (o,c) for o,c in game.objects_eaten.items()])\n",
    "        display(game.to_html([\n",
    "            \"DTW        = %.1f\" % (game.distance_to_walls(),),\n",
    "            \"experience = %d\" % (len(controller.experience),),\n",
    "            \"reward = %.1f\" % (sum(recent_reward)/len(recent_reward),),\n",
    "            \"objects eaten => %s\" % (objects_eaten_str,),\n",
    "\n",
    "        ]))\n",
    "    eq.schedule_recurring(visualize, time_between_frames)\n",
    "        \n",
    "    simulated_up_to = time.time()\n",
    "\n",
    "    def simulate_game():\n",
    "        nonlocal simulated_up_to\n",
    "        now = time.time()\n",
    "        time_to_be_simulated = speed * (now - simulated_up_to)\n",
    "        for _ in range(int(time_to_be_simulated / simulation_resultion)):\n",
    "            game.step(simulation_resultion)\n",
    "        simulated_up_to = now\n",
    "        \n",
    "    eq.schedule_recurring(simulate_game, min(time_between_frames, time_between_actions))\n",
    "\n",
    "    \n",
    "    last_observation = None\n",
    "    last_action      = None\n",
    "    actions_so_far = 0\n",
    "    \n",
    "    def control():\n",
    "        nonlocal last_observation\n",
    "        nonlocal last_action\n",
    "        nonlocal actions_so_far\n",
    "        # sense\n",
    "        new_observation = game.observe()\n",
    "        reward          = game.collect_reward()\n",
    "        # store last transition\n",
    "        actions_so_far += 1\n",
    "        if last_observation is not None and actions_so_far % store_every_nth == 0:\n",
    "            controller.store(last_observation, last_action, reward, new_observation)\n",
    "        # act\n",
    "        new_action = controller.action(new_observation)\n",
    "        game.perform_action(new_action)\n",
    "        last_action = new_action\n",
    "        last_observation = new_observation\n",
    "        \n",
    "        #train\n",
    "        controller.training_step()\n",
    "        \n",
    "    \n",
    "    eq.schedule_recurring(control, time_between_actions)\n",
    "    \n",
    "    eq.run()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[775.67 125.34] [693.17 165.07]\n",
      "71.5670231046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 1.00, 0.00, 25.44, -29.51, 71.57],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33],\n",
       "       [0.00, 0.00, 1.00, 0.00, 0.00, 14.33]])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.__class__ = KarpathyGame\n",
    "np.set_printoptions(formatter={'float': (lambda x: '%.2f' % (x,))})\n",
    "x = g.observe()\n",
    "new_shape = (x[:-2].shape[0]//g.eye_observation_size, g.eye_observation_size)\n",
    "x[:-2].reshape(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<?xml version=\"1.0\"?>\n",
       "\n",
       "<svg height=\"700\" width=\"820\" >\n",
       "\n",
       " <g style=\"fill-opacity:1.0; stroke:black;\n",
       "\n",
       "  stroke-width:1;\">\n",
       "\n",
       "  <rect x=\"10\" y=\"10\" height=\"600\"\n",
       "\n",
       "        width=\"800\" style=fill:none; />\n",
       "\n",
       "  <circle cx=\"151\" cy=\"154\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"33\" cy=\"113\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"737\" cy=\"427\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"520\" cy=\"347\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"445\" cy=\"419\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"687\" cy=\"180\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"380\" cy=\"95\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"135\" cy=\"235\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"786\" cy=\"351\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"201\" cy=\"442\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"413\" cy=\"521\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"662\" cy=\"129\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"653\" cy=\"526\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"409\" cy=\"25\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"491\" cy=\"264\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"702\" cy=\"175\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"721\" cy=\"478\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"433\" cy=\"535\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"342\" cy=\"483\" r=\"10\"\n",
       "\n",
       "          style=fill:green; />\n",
       "\n",
       "  <circle cx=\"770\" cy=\"474\" r=\"10\"\n",
       "\n",
       "          style=fill:red; />\n",
       "\n",
       "  <circle cx=\"784\" cy=\"137\" r=\"10\"\n",
       "\n",
       "          style=fill:yellow; />\n",
       "\n",
       "  <line x1=\"795\" y1=\"137\" x2=\"894\" y2=\"137\" />\n",
       "\n",
       "  <line x1=\"795\" y1=\"139\" x2=\"892\" y2=\"158\" />\n",
       "\n",
       "  <line x1=\"794\" y1=\"141\" x2=\"886\" y2=\"179\" />\n",
       "\n",
       "  <line x1=\"793\" y1=\"143\" x2=\"875\" y2=\"198\" />\n",
       "\n",
       "  <line x1=\"792\" y1=\"145\" x2=\"862\" y2=\"215\" />\n",
       "\n",
       "  <line x1=\"790\" y1=\"146\" x2=\"845\" y2=\"228\" />\n",
       "\n",
       "  <line x1=\"788\" y1=\"147\" x2=\"826\" y2=\"238\" />\n",
       "\n",
       "  <line x1=\"786\" y1=\"148\" x2=\"805\" y2=\"245\" />\n",
       "\n",
       "  <line x1=\"784\" y1=\"148\" x2=\"784\" y2=\"247\" />\n",
       "\n",
       "  <line x1=\"782\" y1=\"148\" x2=\"763\" y2=\"245\" />\n",
       "\n",
       "  <line x1=\"780\" y1=\"147\" x2=\"742\" y2=\"238\" />\n",
       "\n",
       "  <line x1=\"778\" y1=\"146\" x2=\"723\" y2=\"228\" />\n",
       "\n",
       "  <line x1=\"776\" y1=\"145\" x2=\"706\" y2=\"215\" />\n",
       "\n",
       "  <line x1=\"775\" y1=\"143\" x2=\"693\" y2=\"198\" />\n",
       "\n",
       "  <line x1=\"774\" y1=\"141\" x2=\"682\" y2=\"179\" />\n",
       "\n",
       "  <line x1=\"773\" y1=\"139\" x2=\"676\" y2=\"158\" />\n",
       "\n",
       "  <line x1=\"773\" y1=\"137\" x2=\"674\" y2=\"137\" />\n",
       "\n",
       "  <line x1=\"773\" y1=\"135\" x2=\"676\" y2=\"115\" />\n",
       "\n",
       "  <line x1=\"774\" y1=\"133\" x2=\"682\" y2=\"95\" />\n",
       "\n",
       "  <line x1=\"775\" y1=\"131\" x2=\"693\" y2=\"76\" />\n",
       "\n",
       "  <line x1=\"776\" y1=\"129\" x2=\"706\" y2=\"59\" />\n",
       "\n",
       "  <line x1=\"778\" y1=\"128\" x2=\"723\" y2=\"45\" />\n",
       "\n",
       "  <line x1=\"780\" y1=\"127\" x2=\"742\" y2=\"35\" />\n",
       "\n",
       "  <line x1=\"782\" y1=\"126\" x2=\"763\" y2=\"29\" />\n",
       "\n",
       "  <line x1=\"784\" y1=\"126\" x2=\"784\" y2=\"27\" />\n",
       "\n",
       "  <line x1=\"786\" y1=\"126\" x2=\"805\" y2=\"29\" />\n",
       "\n",
       "  <line x1=\"788\" y1=\"127\" x2=\"826\" y2=\"35\" />\n",
       "\n",
       "  <line x1=\"790\" y1=\"128\" x2=\"845\" y2=\"45\" />\n",
       "\n",
       "  <line x1=\"792\" y1=\"129\" x2=\"862\" y2=\"59\" />\n",
       "\n",
       "  <line x1=\"793\" y1=\"131\" x2=\"875\" y2=\"76\" />\n",
       "\n",
       "  <line x1=\"794\" y1=\"133\" x2=\"886\" y2=\"95\" />\n",
       "\n",
       "  <line x1=\"795\" y1=\"135\" x2=\"892\" y2=\"115\" />\n",
       "\n",
       "  <text x=\"10\" y=\"635\" font-size=\"15\">\n",
       "\n",
       "   DTW        = 15.5\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"655\" font-size=\"15\">\n",
       "\n",
       "   experience = 23833\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"675\" font-size=\"15\">\n",
       "\n",
       "   reward = -0.0\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"695\" font-size=\"15\">\n",
       "\n",
       "   objects eaten => friend: 1854, enemy: 1852\n",
       "\n",
       "  </text>\n",
       "\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<svg.Scene at 0x7fb1613ae860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-321-8ea080487c82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulation_resultion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_per_second\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstore_every_nth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-316-3870a7479f55>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(game, controller, fps, actions_per_second, simulation_resultion, speed, store_every_nth)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0meq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule_recurring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_between_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0meq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sidor/projects/dali-deepq/event_queue.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simulate(g, c, fps = 30, simulation_resultion=0.01, actions_per_second=10, speed=0.5, store_every_nth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERERERERERERERERERERERDrF/wFmj2E6fHNDpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb15cb3b940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g.plot_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIiK5/D/vBKZqars7FQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb15ee2f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g.plot_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.568132557129275"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([775.67, 125.34])\n",
    "y = np.array([693.17, 165.07])\n",
    "\n",
    "np.sqrt(sum((x - y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SETTINGS[\"object_radius\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.568132557129275"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_distance(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = Point(3.41312321321321,4.1234123421342134) + Point(1,2.55132413243214321432143214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.41312321321321"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(t.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Circle(Point(0,0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Segment(Point(0,0), Point(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Point(sqrt(2)/4 + 1/2, sqrt(2)/4 + 1/2)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c.intersection(s)[0] + Point(1,1))/ 4 + Point(1,1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Point(-1,-1) in Polygon(Point(0,0), Point(0,1), Point(1,1), Point(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only two dimensional points currently supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-367-374c3022ee10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/sympy/geometry/point.py\u001b[0m in \u001b[0;36mdistance\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \"\"\"\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/sympy/geometry/point.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 raise ValueError(\n\u001b[1;32m---> 86\u001b[1;33m                     \"Only two dimensional points currently supported\")\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only two dimensional points currently supported"
     ]
    }
   ],
   "source": [
    "Point(-1,-1).distance(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqrt(2)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.distance(Point(-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
